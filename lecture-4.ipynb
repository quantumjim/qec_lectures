{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbd5a61",
   "metadata": {},
   "source": [
    "# Lecture 4: Programming HDRG Decoders\n",
    "### James R. Wootton, IBM Quantum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24236e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit_qec.utils import Decodoku\n",
    "from qiskit_qec.decoders import DecodingGraph\n",
    "from rustworkx.visualization import mpl_draw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69663d36",
   "metadata": {},
   "source": [
    "## Beyond matching with qudit codes\n",
    "\n",
    "We usually consider doing quantum computing with qubits: two-level quantum systems. But we could also do it with higher dimensional systems. We call these *qudits*. We can label the states `0`, `1`, ..., `k-1`, where $k$ here is the qudit dimension.\n",
    "\n",
    "Just as we can define things like surface codes and repetition codes for qubits, we can do so for qudits too. Consider again the repetition code as a simple example. Before our syndrome measurements had two possible outcomes:\n",
    "    * `0` implies neighbouring code qubits in the same state (`00` or `11`);\n",
    "    * `1` implies neighbouring code qubits in different states (`01` or `01`).\n",
    "By extracting only this information, we learn nothing about whether the code qubits are individually `0` or `1`, which means that the syndrome measurement doesn't destroy superpositions.\n",
    "\n",
    "For `k=3` qudits, there are more ways that the code qudits can differ. Our syndrome can give us this additional information.\n",
    "* `0` implies neighbouring code qubits in the same state (`00`, `11` or `22`);\n",
    "* `1` implies the right one is one higher than the left (`01`, `12` or `20`);\n",
    "* `2` implies the right one is two higher than the left (`02`, `10` or `21`);\n",
    "\n",
    "Let's consider a few examples of what this looks like in practice. Here's 7 (qu)dits encoding a logical `0`, and the resulting (trivial) syndrome.\n",
    "\n",
    "`0000000` $\\,\\, \\rightarrow \\,\\,$ `000000`\n",
    "\n",
    "Now suppose an error flips one of the values. The two affected syndromes will give non-trivial values. But they'll give differnt values depending on the nature of the flip. Here's an example of a $+1$ flip, which adds $1$ (mod 3).\n",
    "\n",
    "`0010000` $\\,\\, \\rightarrow \\,\\,$ `012000`\n",
    "\n",
    "And here's a $+2$ flip in a different position.\n",
    "\n",
    "`0002000` $\\,\\, \\rightarrow \\,\\,$ `002100`\n",
    "\n",
    "Now let's look at what happens when we have both.\n",
    "\n",
    "`0012000` $\\,\\, \\rightarrow \\,\\,$ `011100`\n",
    "\n",
    "Note that the syndrome here has three non-trivial values. For this code, highlighted nodes are not created in pairs. Instead they are created in clusters, for which the values sum to 3.\n",
    "\n",
    "In general for such codes, sets of errors create a cluster of highlighted nodes whose values add up to a multiple of $k$.\n",
    "\n",
    "For these codes, we can't just use matching. So let's have a look at the kind of algorithms that do work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=3,p=0.1)\n",
    "\n",
    "game.draw_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de5b21",
   "metadata": {},
   "source": [
    "As ever, it will be useful to have a notion of distance between nodes. Let's with the standard Manhattan distance. In the Decodoku problems, this is also the number of nodes connecting edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b92da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manhattan_distance(graph, n0, n1):\n",
    "    node0 = graph[n0]\n",
    "    node1 = graph[n1]\n",
    "    dx = abs(node0['x']-node1['x'])\n",
    "    if node0['is_boundary'] or node1['is_boundary']:\n",
    "        dy = 0\n",
    "    else:\n",
    "        dy = abs(node0['y']-node1['y'])\n",
    "    return dx + dy\n",
    "\n",
    "get_distance = get_manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335d63f",
   "metadata": {},
   "source": [
    "Now we'll again do something much like we did for matching: set up a new graph whose structure depends on where the highlighted nodes are.\n",
    "\n",
    "This we'll make what we'll call a `cluster_graph`. This is basically the same as the `error_graph` from last time, but we'll leave all the non-highlighted nodes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10143047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum distance\n",
    "dist_max = 1\n",
    "\n",
    "# get useful info\n",
    "d = game.d\n",
    "dg = game.decoding_graph.graph\n",
    "\n",
    "# create empty `DecodingGraph`\n",
    "cluster_graph = DecodingGraph(None)\n",
    "cg = cluster_graph.graph\n",
    "\n",
    "# add all nodes from dg\n",
    "for node in dg.nodes():\n",
    "    cg.add_node(node)\n",
    "    if node['is_boundary']:\n",
    "        # set boundary values to be nan\n",
    "        node['value'] = np.nan\n",
    "\n",
    "# add an edge between a pair of nodes if their distance is small enough\n",
    "for n0, node0 in enumerate(dg.nodes()):\n",
    "    for n1, node1 in enumerate(dg.nodes()):\n",
    "        if n0<n1:\n",
    "            dist = get_distance(dg,n0,n1)\n",
    "            if dist<=dist_max:\n",
    "                if (node0['highlighted'] or node0['is_boundary']) and (node1['highlighted'] or node1['is_boundary']):\n",
    "                    cg.add_edge(n0, n1, {'distance':get_distance(dg,n0,n1)})\n",
    "\n",
    "mpl_draw(cg, pos=game.graph_pos, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27034050",
   "metadata": {},
   "source": [
    "We'll now find the connected components of this graph, and then see if the parity of all their nodes is neutral. If so, we can conclude that they were created by the same set of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137b9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rustworkx import connected_components\n",
    "\n",
    "# find the connected components of cg\n",
    "con_comps = connected_components(cg)\n",
    "\n",
    "# use these to define clusters\n",
    "clusters = {}\n",
    "parity = {}\n",
    "for c,con_comp in enumerate(con_comps):\n",
    "\n",
    "    # check the parity of each connected component\n",
    "    parity[c] = 0\n",
    "    for n in con_comp:\n",
    "        parity[c] += dg[n]['value']\n",
    "    parity[c] = parity[c]%game.k\n",
    "    # note that boundary connected components will have a parity of nan\n",
    "\n",
    "    # if neutral (or boundary connected), it's a cluster\n",
    "    # otherwise we tag highlighed nodes as being part of non-neutral clusters\n",
    "    for n in con_comp:\n",
    "        if dg[n]['highlighted'] or dg[n]['is_boundary']:\n",
    "            if parity[c]==0 or parity[c]!=parity[c]:\n",
    "                clusters[n] = c\n",
    "            else:\n",
    "                clusters[n] = 'non-neutral'\n",
    "        else:\n",
    "            clusters[n] = None\n",
    "\n",
    "# assign colours to nodes based on their status:\n",
    "# * a different colour for each node\n",
    "# * red for highlighted nodes not in neutral clusters\n",
    "# * light grey for the rest\n",
    "node_color = ['#bbbbbb']*len(dg)\n",
    "for n in clusters:\n",
    "    if clusters[n] is not None:\n",
    "        if clusters[n]=='non-neutral':\n",
    "            node_color[n] = 'red'\n",
    "        else:\n",
    "            node_color[n] = game.error_colors[clusters[n]]\n",
    "\n",
    "mpl_draw(cg, pos=game.graph_pos, node_color=node_color, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b368610b",
   "metadata": {},
   "source": [
    "Once we've decided that these edges contain errors, we could use this information to modify the decoding graph. Effectively we combine these clusters into a single neutral node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a77a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_dg = dg.copy()\n",
    "# copy the positions for dg and add some empty spaces\n",
    "reduced_pos = game.graph_pos + [None]*100\n",
    "\n",
    "for c,con_comp in enumerate(con_comps):\n",
    "    if len(con_comp)>1:\n",
    "        con_comp = list(con_comp)\n",
    "        new_n = reduced_dg.contract_nodes(con_comp,c)\n",
    "        if parity[c]==0:\n",
    "            reduced_pos[new_n] = reduced_pos[con_comp[0]]\n",
    "        if parity[c]!=parity[c]:\n",
    "            for n in con_comp:\n",
    "                if dg[n]['is_boundary']:\n",
    "                    reduced_pos[new_n] = reduced_pos[n]\n",
    "\n",
    "mpl_draw(reduced_dg, pos=reduced_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb76b78",
   "metadata": {},
   "source": [
    "There are various decoders that we could define that are based on this kind of information. So let's combine all this into a handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d33478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(game, dist_max):\n",
    "\n",
    "    # get useful info\n",
    "    d = game.d\n",
    "    dg = game.decoding_graph.graph\n",
    "\n",
    "    # create empty `DecodingGraph`\n",
    "    cluster_graph = DecodingGraph(None)\n",
    "    cg = cluster_graph.graph\n",
    "\n",
    "    # add all nodes from dg\n",
    "    for node in dg.nodes():\n",
    "        cg.add_node(node)\n",
    "        if node['is_boundary']:\n",
    "            # set boundary values to be nan\n",
    "            node['value'] = np.nan\n",
    "\n",
    "    # add an edge between a pair of nodes if their distance is small enough\n",
    "    for n0, node0 in enumerate(dg.nodes()):\n",
    "        for n1, node1 in enumerate(dg.nodes()):\n",
    "            if n0<n1:\n",
    "                dist = get_distance(dg,n0,n1)\n",
    "                if dist<=dist_max:\n",
    "                    if (node0['highlighted'] or node0['is_boundary']) and (node1['highlighted'] or node1['is_boundary']):\n",
    "                        cg.add_edge(n0, n1, {'distance':get_distance(dg,n0,n1)})\n",
    "\n",
    "    # find the connected components of cg\n",
    "    con_comps = connected_components(cg)\n",
    "\n",
    "    # use these to define clusters\n",
    "    clusters = {}\n",
    "    parity = {}\n",
    "    for c,con_comp in enumerate(con_comps):\n",
    "\n",
    "        # check the parity of each connected component\n",
    "        parity[c] = 0\n",
    "        for n in con_comp:\n",
    "            parity[c] += dg[n]['value']\n",
    "        parity[c] = parity[c]%game.k\n",
    "        # note that boundary connected components will have a parity of nan\n",
    "\n",
    "        # if neutral (or boundary connected), it's a cluster\n",
    "        # otherwise we tag highlighed nodes as being part of non-neutral clusters\n",
    "        for n in con_comp:\n",
    "            if dg[n]['highlighted'] or dg[n]['is_boundary']:\n",
    "                if parity[c]==0 or parity[c]!=parity[c]:\n",
    "                    clusters[n] = c\n",
    "                else:\n",
    "                    clusters[n] = 'non-neutral'\n",
    "            else:\n",
    "                clusters[n] = None\n",
    "\n",
    "    # assign colours to nodes based on their status:\n",
    "    # * a different colour for each node\n",
    "    # * red for highlighted nodes not in neutral clusters\n",
    "    # * dark grey for the rest\n",
    "    node_color = ['#bbbbbb']*len(dg)\n",
    "    for n in clusters:\n",
    "        if clusters[n] is not None:\n",
    "            if clusters[n]=='non-neutral':\n",
    "                node_color[n] = 'red'\n",
    "            else:\n",
    "                node_color[n] = game.error_colors[clusters[n]]\n",
    "\n",
    "    reduced_dg = dg.copy()\n",
    "    # copy the positions for dg and add some empty spaces\n",
    "    reduced_pos=game.graph_pos + [None]*100\n",
    "\n",
    "    for c,con_comp in enumerate(con_comps):\n",
    "        if len(con_comp)>1:\n",
    "            con_comp = list(con_comp)\n",
    "            new_n = reduced_dg.contract_nodes(con_comp,c)\n",
    "            if parity[c]==0:\n",
    "                reduced_pos[new_n] = reduced_pos[con_comp[0]]\n",
    "            if parity[c]!=parity[c]:\n",
    "                for n in con_comp:\n",
    "                    if dg[n]['is_boundary']:\n",
    "                        reduced_pos[new_n] = reduced_pos[n]\n",
    "\n",
    "    return cg, node_color, reduced_dg, reduced_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7edb9c7",
   "metadata": {},
   "source": [
    "## The basic flow of HDRG decoders\n",
    "\n",
    "Hard decision renormalization group decoders use an increasing search distance to look for how to explain highlighted nodes, and then make final decisions on how to do this. Once everything had been explained, they stop.\n",
    "\n",
    "With the tools we have so far, let's look at some decoding problems to get a sense of how HDRG decoders work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=3,p=0.08,d=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff004337",
   "metadata": {},
   "source": [
    "First we should look at the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37318308",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.draw_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d61ee",
   "metadata": {},
   "source": [
    "Then let's look at how they cluster for a small distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56041399",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_max = 1\n",
    "cg, node_color, reduced_dg, reduced_pos = cluster(game, dist_max) \n",
    "\n",
    "mpl_draw(cg, pos=game.graph_pos, node_color=node_color, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e0437",
   "metadata": {},
   "source": [
    "We can then clean up all the neutral clusters found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c20138",
   "metadata": {},
   "outputs": [],
   "source": [
    "game.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f684c24b",
   "metadata": {},
   "source": [
    "Before we continue, it might be good to keep in mind how this has affected the syndrome graph (since further analysis by the function here will have forgotten it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd9f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl_draw(reduced_dg, pos=reduced_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e3a9c",
   "metadata": {},
   "source": [
    "Now let's head back up to the clustering and increase `dist_max`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce1eb6",
   "metadata": {},
   "source": [
    "## The history of HDRG decoders\n",
    "\n",
    "Here's a list important papers (slightly biased towards the person writing the list).\n",
    "* 2011 **Bravyi-Haah**: [Bravyi and Haah](https://arxiv.org/abs/1112.3252).\n",
    "* 2004 and 2013 **Expanding diamonds**: [Harrington](https://thesis.library.caltech.edu/1747/), [Dennis](arXiv:quant-ph/0503169) and [Wootton](https://arxiv.org/abs/1310.2393).\n",
    "* 2013 **ABCB**: [Anwar, Brown, Campbell, Browne](https://arxiv.org/abs/1311.4895).\n",
    "* 2014 **Shortcuts and matching**: [Hutter, Loss and Wootton](https://arxiv.org/abs/1410.4478).\n",
    "* 2017 **Weasel**: [Wootton](https://arxiv.org/abs/1712.09649) (inspired by results from the Decodoku citizen science project).\n",
    "* 2017 **Union-Find**: [Delfosse and Nickerson](https://arxiv.org/abs/1709.06218).\n",
    "\n",
    "The algorithms these present differ in how they increase the search distance, and how they account for clusters that have already been removed.\n",
    "\n",
    "### Bravyi-Haah, Expanding diamonds and ABCB\n",
    "\n",
    "For *expanding diamonds*, it's best to go back to the qubit case and do it manually. In this we increase a search distance, and remove neutral clusters as soon as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c61ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=2,p=0.1)\n",
    "game.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4abb5ea",
   "metadata": {},
   "source": [
    "For *Bravyi-Haah* we use the Chebyshev distance rather than Manhattan, which means $\\max(dx,dy)$ instead of $dx+dy$. Also, we increase the search distance exponentially. We can implement this by redfining the distance below and going back to the basic flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chebyshev_distance(graph, n0, n1):\n",
    "    node0 = graph[n0]\n",
    "    node1 = graph[n1]\n",
    "    dx = abs(node0['x']-node1['x'])\n",
    "    if node0['is_boundary'] or node1['is_boundary']:\n",
    "        dy = 0\n",
    "    else:\n",
    "        dy = abs(node0['y']-node1['y'])\n",
    "    return max(dx,dy)\n",
    "\n",
    "get_distance = get_chebyshev_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38898ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=3,p=0.05,d=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c88b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_max = 2**0\n",
    "cg, node_color, reduced_dg, reduced_pos = cluster(game, dist_max) \n",
    "\n",
    "mpl_draw(cg, pos=game.graph_pos, node_color=node_color, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e11379",
   "metadata": {},
   "source": [
    "The ABCB decoder uses the Manhattan distance and a linearly increasing distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e5512",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_distance = get_manhattan_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886745b",
   "metadata": {},
   "source": [
    "### Cantor-like error chains, shortcuts and Weasel\n",
    "\n",
    "Suppose we have the following set of errors, expressed in ASCII art. Here `*` is a highlighted node, `-` is an error and the numbers represent distances.\n",
    "\n",
    "```\n",
    "      2    2       2    2                2    2       2    2 \n",
    "    *--* *--*    *--* *--*             *--* *--*    *--* *--*\n",
    "        1      4     1           13        1      4     1    \n",
    "```\n",
    "\n",
    "The HDRG decoders we've seen so far would typically notice the pairs separated by distance 1 and 'correct' them. But actually, this would create longer error chains.\n",
    "\n",
    "```\n",
    "      2    2       2    2                2    2       2    2 \n",
    "    *--·~·--*    *--·~·--*             *--·~·--*    *--·~·--*\n",
    "        1      4     1           13        1      4     1    \n",
    "```\n",
    "\n",
    "Then they'd do the same to those separated by distance 4.\n",
    "\n",
    "```\n",
    "      2    2       2    2                2    2       2    2 \n",
    "    *--·~·--·~~~~·--·~·--*             *--·~·--·~~~~·--·~·--*\n",
    "        1      4     1           13        1      4     1    \n",
    "```\n",
    "\n",
    "And so on\n",
    "\n",
    "```\n",
    "      2    2       2    2                2    2       2    2 \n",
    "    *--·~·--·~~~~·--·~·--·~~~~~~~~~~~~~·--·~·--·~~~~·--·~·--*\n",
    "        1      4     1           13        1      4     1    \n",
    "```\n",
    "\n",
    "These Cantor-like error chains can be a big problem, since they can cause logical errors with much less than $d/2$ physical errors.\n",
    "\n",
    "Specifically, they are made up by a copying a structure. Between the copies we put a gap that is slightly smaller than the width of the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# number of errors in each level\n",
    "errors = [2]\n",
    "# length of each level\n",
    "length = [2]\n",
    "\n",
    "# repeat the process for multiple levels\n",
    "for _ in range(20):\n",
    "\n",
    "    # each level has twice the errors as the last\n",
    "    errors.append(errors[-1]*2)\n",
    "    # and (almost) 3x the distance\n",
    "    length.append(2*length[-1] + length[-1]-1)\n",
    "\n",
    "plt.scatter(length,errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a415d5",
   "metadata": {},
   "source": [
    "Assuming that the number of errors $n_e$ and the length of the error chain $l$ are related according to $n_e \\sim l^\\beta$, then\n",
    "\n",
    "$\\log{n_e} \\sim \\log{l^\\beta} = \\beta \\log{l}$.\n",
    "\n",
    "So we'll be able to find $\\beta$ as the gradient of a log-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6283f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.log(length),np.log(errors))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da344baa",
   "metadata": {},
   "source": [
    "Clearly $\\beta < 1$. So our effective code distance is no longer $d$, but scales as $d^\\beta$. Just because of the decoder!\n",
    "\n",
    "One way to get around this is to use 'shortcuts': take into account the reduced distances as clusters are combined.\n",
    "\n",
    "Another way, used by the *Weasel* decoder, is to keep neutral clusters around. The method here is basically:\n",
    "* Each non-neutral cluster merges with its nearest neighbouring cluster (neutral or non-neutral);\n",
    "* Repeat. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aee8bfd",
   "metadata": {},
   "source": [
    "## Non-Abelian decoding\n",
    "\n",
    "So far we've been able to use a very important piece of information: we know how the syndrome would change if we modified the set of errors. But that's not always the case, such as with codes whose syndromes are described by non-Abelian anyons.\n",
    "\n",
    "The simplest are the [non-cyclic anyon models](https://arxiv.org/abs/1607.02159), in which information can be fully resolved in a finite number of rounds (assuming no new errors).\n",
    "\n",
    "An example of the kind of behaviour found for Ising anyons can be seen in the problem below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee11634",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=4,p=0.05,d=11,nonabelian=True)\n",
    "\n",
    "game.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757c1367",
   "metadata": {},
   "source": [
    "More complex is the decoding of non-cyclic anyon models, as described [here](https://arxiv.org/pdf/1506.00524.pdf). It was only recently that it was shown that a threshold even exists for such models, in [this paper](https://arxiv.org/abs/2012.04610)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49cb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Decodoku(k=6,p=0.05,d=11,nonabelian=True)\n",
    "\n",
    "game.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2350b796",
   "metadata": {},
   "source": [
    "## Open Problems\n",
    "\n",
    "A few open problems related to the issues discussed here.\n",
    "\n",
    "* What is the minimal fatal error for the Weasel decoder?\n",
    "* How well does Union Find work for:\n",
    "    * Qudit codes?\n",
    "    * Non-Abelian decoding?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('decodoku')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "3631a5c11ea8272419473841f6c7a371f0144b233b78c9dd4753c6019b6bfd67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
